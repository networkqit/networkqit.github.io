

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>networkqit.algorithms.optimize &mdash; networkqit beta documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script type="text/javascript" src="../../../_static/copybutton.js"></script>
        <script async="async" type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within networkqit beta documentation"
          href="../../../_static/opensearch.xml"/>

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> networkqit
          

          
          </a>

          
            
            
              <div class="version">
                0.3
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorial.html">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/infotheory.html">Information theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/graphtheory.html">Graph theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/algorithms.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../citing.html">Citing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../citing.html#references">References</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">networkqit</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>networkqit.algorithms.optimize</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for networkqit.algorithms.optimize</h1><div class="highlight"><pre>
<span></span><span class="ch">#! /usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1">#</span>
<span class="c1"># networkqit -- a python module for manipulations of spectral entropies framework</span>
<span class="c1">#</span>
<span class="c1"># Copyright (C) 2017-2018 Carlo Nicolini &lt;carlo.nicolini@iit.it&gt;</span>
<span class="c1">#</span>
<span class="c1"># This program is free software: you can redistribute it and/or modify</span>
<span class="c1"># it under the terms of the GNU General Public License as published by</span>
<span class="c1"># the Free Software Foundation, either version 3 of the License, or</span>
<span class="c1"># (at your option) any later version.</span>
<span class="c1">#</span>
<span class="c1"># This program is distributed in the hope that it will be useful,</span>
<span class="c1"># but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="c1"># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="c1"># GNU General Public License for more details.</span>
<span class="c1">#</span>
<span class="c1"># You should have received a copy of the GNU General Public License</span>
<span class="c1"># along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>

<span class="sd">&quot;&quot;&quot;</span>

<span class="sd">Define the base and inherited classes for model optimization, both in the continuous approximation</span>
<span class="sd">and for the stochastic optimization.</span>

<span class="sd">The `ModelOptimizer` class defines the base class where all the other optimization classes must inherit.</span>
<span class="sd">The most important class to optimize an expected adjacency model is `ContinuousOptimizer`.</span>
<span class="sd">In this class the gradients are defined as:</span>

<span class="sd">.. math::</span>
<span class="sd">    </span>
<span class="sd">        \\frac{\\partial S(\\boldsymbol \\rho \\| \\boldsymbol \\sigma(\\mathbb{E}_{\\boldsymbol \\theta}[\\mathbf{L}]))}{\\partial \\boldsymbol \\theta_k} = \\beta \\textrm{Tr}\\biggl \\lbrack \\left(\\boldsymbol \\rho - \\boldsymbol \\sigma(\\mathbb{E}_{\\boldsymbol \\theta}[L])\\right)\\frac{\\partial \\mathbb{E}_{\\boldsymbol \\theta}[\mathbf{L}]}{\\partial \\theta_k} \\biggr \\rbrack</span>

<span class="sd">In the `StochasticOptimizer` class we instead address the issue to implement stochastic gradient descent methods.</span>
<span class="sd">In these methods the gradients are defined as:</span>

<span class="sd">.. math::</span>
<span class="sd">   </span>
<span class="sd">   \\frac{\\partial \\mathbb{E}_{\\boldsymbol \\theta}[S(\\boldsymbol \\rho \\| \\boldsymbol \\sigma)]}{\\partial \\theta_k} = \\beta \\textrm{Tr}\\biggl \\lbrack \\boldsymbol \\rho \\frac{\\partial  \\mathbb{E}_{\\boldsymbol \\theta}[L]}{\\partial \\boldsymbol \\theta_k}\\biggr \\rbrack + \\frac{\\partial}{\\partial \\theta_k}\\mathbb{E}_{\\boldsymbol \\theta}\\biggl \\lbrack \\log \\left( \\textrm{Tr}\\left \\lbrack e^{-\\beta L(\\boldsymbol \\theta)} \\right \\rbrack \\right) \\biggr \\rbrack</span>

<span class="sd">The stochastic optimizer is the **correct** optimizer, as it makes no approximation on the Laplacian eigenvalues.</span>
<span class="sd">It is more suitable for small graphs and intermediate $\\beta$, where the differences between the random matrix s</span>
<span class="sd">pectrum and its expected counterpart are non-neglibile.</span>
<span class="sd">For large and dense enough graphs however the `ContinuousOptimizer` works well and yields deterministic results,</span>
<span class="sd">as the optimization landscape is smooth.</span>

<span class="sd">In order to minimize the expected relative entropy then, we need both the expected Laplacian formula, which is simple</span>
<span class="sd">to get, and a way to estimate the second summand in the gradient, that involves averaging over different realizations</span>
<span class="sd">of the log trace of  $e^{-\\beta L(\\boldsymbol \\theta)}$.</span>
<span class="sd">A good the approximation to the expected logtrace $\\mathbb{E}_{\\boldsymbol \\theta}[\\log \\textrm{Tr}[\\exp{(-\\beta L)}]]$ is,</span>
<span class="sd">makes a better is the estimate of the gradients.</span>

<span class="sd">Finally, the `MLEOptimizer` maximizes the standard likelihood of a model and it is not related to the spectral entropies</span>
<span class="sd">framework introduced in the paper on which **networkqit** is based.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="c1">#    Copyright (C) 2018 by</span>
<span class="c1">#    Carlo Nicolini &lt;carlo.nicolini@iit.it&gt;</span>
<span class="c1">#    All rights reserved.</span>
<span class="c1">#    BSD license.</span>

<span class="kn">import</span> <span class="nn">autograd</span>
<span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">autograd.numpy.linalg</span> <span class="k">import</span> <span class="n">eigh</span>
<span class="kn">from</span> <span class="nn">autograd.scipy.misc</span> <span class="k">import</span> <span class="n">logsumexp</span>
<span class="kn">from</span> <span class="nn">networkqit.graphtheory.matrices</span> <span class="k">import</span> <span class="n">softmax</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="k">import</span> <span class="n">minimize</span><span class="p">,</span> <span class="n">least_squares</span><span class="p">,</span> <span class="n">OptimizeResult</span>
<span class="kn">from</span> <span class="nn">networkqit.graphtheory</span> <span class="k">import</span> <span class="o">*</span> <span class="c1"># imports GraphModel</span>
<span class="kn">from</span> <span class="nn">networkqit.infotheory.density</span> <span class="k">import</span> <span class="o">*</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;MLEOptimizer&#39;</span><span class="p">,</span>
           <span class="s1">&#39;ContinuousOptimizer&#39;</span><span class="p">,</span>
           <span class="s1">&#39;StochasticOptimizer&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Adam&#39;</span><span class="p">]</span>

<div class="viewcode-block" id="MLEOptimizer"><a class="viewcode-back" href="../../../reference/generated/networkqit.algorithms.optimize.MLEOptimizer.html#networkqit.algorithms.optimize.MLEOptimizer">[docs]</a><span class="k">class</span> <span class="nc">MLEOptimizer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class, inheriting from the model optimizer class solves the problem of </span>
<span class="sd">    maximum likelihood parameters estimation in the classical settings.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="MLEOptimizer.__init__"><a class="viewcode-back" href="../../../reference/generated/networkqit.algorithms.optimize.MLEOptimizer.html#networkqit.algorithms.optimize.MLEOptimizer.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">G</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">x0</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">GraphModel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the optimizer with the observed graph, an initial guess and the </span>
<span class="sd">        model to optimize.</span>

<span class="sd">        Args:</span>
<span class="sd">            G (numpy.array) :is the empirical network to study. A N x N adjacency matrix as numpy.array.</span>
<span class="sd">            x0 (numpy.array): is the k-element array of initial parameters estimates. Typically set as random.</span>
<span class="sd">            model (nq.GraphModel): the graph model to optimize the likelihood of.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">G</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x0</span> <span class="o">=</span> <span class="n">x0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sol</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">ftol</span><span class="o">=</span><span class="mf">1E-10</span><span class="p">,</span> <span class="n">gtol</span><span class="o">=</span><span class="mf">1E-5</span><span class="p">,</span> <span class="n">xtol</span><span class="o">=</span><span class="mf">1E-8</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mf">1E4</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Maximimize the likelihood of the model given the observed network G.</span>

<span class="sd">        Args:</span>
<span class="sd">            method (str): optimization method to use. Can be either &#39;MLE&#39; or &#39;saddle_point&#39;.</span>
<span class="sd">                          &#39;MLE&#39; uses L-BFGS-B method to optimize the likelihood of the model</span>
<span class="sd">                          &#39;saddle_point&#39; finds the saddle point solution solving a system of</span>
<span class="sd">                          nonlinear equation. This method finds the parameters such that the</span>
<span class="sd">                          gradients are zero. Here we adopt two different methods.</span>
<span class="sd">                          Models leading to convex likelihood are solved by a direct call to</span>
<span class="sd">                          the least-square optimizer, where we minimize the square error cost:</span>
<span class="sd">                          $sum_i (c_i^* - &lt;c_i&gt;)^2$</span>
<span class="sd">                          where c_i is the empirical value of the specific constraint from MaxEnt</span>
<span class="sd">                          and &lt;c_i&gt; is its ensemble average as defined from the model.</span>
<span class="sd">                          Empirically we found that the &#39;dogbox&#39; method works best in most cases</span>
<span class="sd">                          with a nice tradeoff between speed and precision.</span>

<span class="sd">            xtol:       (float) set the termination tolerance on parameters</span>
<span class="sd">                        (default 1E-10)</span>

<span class="sd">            gtol:       (float) set the termination tolerance on gradients</span>
<span class="sd">                        (default 1E-10)</span>

<span class="sd">            maxiter:    (int) set the maximum number of iteration. Default 10.000 iterations</span>

<span class="sd">        Kwargs:</span>
<span class="sd">            maxfun:     (int) set the maximum number of evaluations of the cost.</span>
<span class="sd">                        Default value is very large ten thousand (1E4)</span>

<span class="sd">            verbose:    (int) Verbosity level. No output=0, 2=iterations output.</span>

<span class="sd">            basin_hopping: (bool) whether to run global optimization with local</span>
<span class="sd">                optimization by least_squares (only for saddle point method)</span>

<span class="sd">            basin_hopping_niter: (int) number of basin hopping repetitions</span>

<span class="sd">        Outputs:</span>
<span class="sd">            sol: (scipy.optimize.OptimizeResult) parameters at optimal likelihood</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">opts</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;ftol&#39;</span><span class="p">:</span> <span class="n">ftol</span><span class="p">,</span>
                <span class="s1">&#39;gtol&#39;</span><span class="p">:</span> <span class="n">gtol</span><span class="p">,</span> <span class="c1"># as default of LBFGSB</span>
                <span class="s1">&#39;xtol&#39;</span><span class="p">:</span> <span class="n">xtol</span><span class="p">,</span>
                <span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="n">maxiter</span><span class="p">,</span>
                <span class="s1">&#39;eps&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="mf">1E-8</span><span class="p">),</span> <span class="c1"># as default of LBFGSB</span>
                <span class="s1">&#39;maxfun&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;maxfun&#39;</span><span class="p">,</span> <span class="mf">1E10</span><span class="p">),</span>
                <span class="s1">&#39;verbose&#39;</span> <span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="s1">&#39;disp&#39;</span><span class="p">:</span>  <span class="nb">bool</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
                <span class="s1">&#39;iprint&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="s1">&#39;maxls&#39;</span> <span class="p">:</span> <span class="mi">100</span>
                <span class="p">}</span>

        <span class="k">if</span> <span class="n">method</span> <span class="ow">is</span> <span class="s1">&#39;MLE&#39;</span><span class="p">:</span>
            <span class="c1"># If the model has non-linear constraints, must use Sequential Linear Square Programming SLSQP</span>
            <span class="kn">from</span> <span class="nn">autograd</span> <span class="k">import</span> <span class="n">jacobian</span> <span class="c1"># using automatic jacobian from autograd</span>
            <span class="n">J</span> <span class="o">=</span> <span class="n">jacobian</span><span class="p">(</span><span class="k">lambda</span> <span class="n">z</span> <span class="p">:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loglikelihood</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">,</span><span class="n">z</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;constraints&#39;</span><span class="p">):</span>
                <span class="c1">#H=hessian(lambda z : -self.model.loglikelihood(self.G,z))</span>
                <span class="c1"># remove some options to avoid warnings</span>
                <span class="n">opts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;gtol&#39;</span><span class="p">),</span> <span class="n">opts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">),</span> <span class="n">opts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;xtol&#39;</span><span class="p">),</span><span class="n">opts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;maxfun&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sol</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loglikelihood</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span>
                                    <span class="n">x0</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">),</span>
                                    <span class="n">method</span><span class="o">=</span><span class="s1">&#39;SLSQP&#39;</span><span class="p">,</span>
                                    <span class="n">constraints</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;fun&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">constraints</span><span class="p">,</span> <span class="s1">&#39;type&#39;</span><span class="p">:</span><span class="s1">&#39;ineq&#39;</span><span class="p">},</span>
                                    <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bounds</span><span class="p">,</span>
                                    <span class="n">jac</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                                    <span class="n">tol</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;ftol&#39;</span><span class="p">],</span>
                                    <span class="n">options</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span> <span class="c1"># the model has only bound-constraints, hence use L-BFGS-B</span>
                <span class="c1"># Optimize using L-BFGS-B which typically returns good results</span>
                <span class="n">opts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;xtol&#39;</span><span class="p">),</span> <span class="n">opts</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sol</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loglikelihood</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span>
                                    <span class="n">x0</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">),</span>
                                    <span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span>
                                    <span class="n">jac</span><span class="o">=</span><span class="n">J</span><span class="p">,</span>
                                    <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bounds</span><span class="p">,</span>
                                    <span class="n">options</span><span class="o">=</span><span class="n">opts</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">])</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="s1">&#39;status&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="ne">RuntimeWarning</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">])</span>
                <span class="c1">#raise Exception(&#39;Method did not converge to maximum likelihood: &#39;)</span>
        <span class="k">elif</span> <span class="n">method</span> <span class="ow">is</span> <span class="s1">&#39;saddle_point&#39;</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;constraints&#39;</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Cannot solve saddle_point_equations with non-linear constraints&#39;</span><span class="p">)</span>
            <span class="c1"># Use the Dogbox method to optimize likelihood</span>
            <span class="c1">#ls_bounds = [np.min(np.ravel(self.model.bounds)), np.max(np.ravel(self.model.bounds))]</span>
            <span class="n">ls_bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
            <span class="n">ls_bounds</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">ls_bounds</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
            <span class="n">ls_bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">ls_bounds</span><span class="p">,[</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">),</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="c1"># it is required in a different format</span>
            <span class="k">def</span> <span class="nf">basin_opt</span><span class="p">(</span><span class="o">*</span><span class="n">basin_opt_args</span><span class="p">,</span> <span class="o">**</span><span class="n">basin_opt_kwargs</span><span class="p">):</span>
                <span class="n">opt_result</span> <span class="o">=</span> <span class="n">least_squares</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="n">basin_opt_args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                           <span class="n">x0</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">basin_opt_args</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                                           <span class="n">bounds</span><span class="o">=</span><span class="n">ls_bounds</span><span class="p">,</span>
                                           <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trf&#39;</span><span class="p">,</span>
                                           <span class="n">xtol</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;xtol&#39;</span><span class="p">],</span>
                                           <span class="n">gtol</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;gtol&#39;</span><span class="p">],</span>
                                           <span class="n">tr_solver</span><span class="o">=</span><span class="s1">&#39;lsmr&#39;</span><span class="p">,</span>
                                           <span class="n">max_nfev</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;maxiter&#39;</span><span class="p">],</span>
                                           <span class="n">verbose</span><span class="o">=</span><span class="n">opts</span><span class="p">[</span><span class="s1">&#39;verbose&#39;</span><span class="p">])</span>
                <span class="c1"># use this form with linear loss as in the basinhopping</span>
                <span class="c1"># func argument to be consistent</span>
                <span class="n">opt_result</span><span class="p">[</span><span class="s1">&#39;fun&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">opt_result</span><span class="p">[</span><span class="s1">&#39;fun&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">opt_result</span>

            <span class="c1"># If the model is convex, we simply run least_squares</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;basinhopping&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sol</span> <span class="o">=</span> <span class="n">basin_opt</span><span class="p">(</span><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">saddle_point</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">,</span> <span class="n">z</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># otherwise combine local and global optimization with basinhopping</span>
                <span class="n">nlineq</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">saddle_point</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
                <span class="kn">from</span> <span class="nn">.basinhoppingmod</span> <span class="k">import</span> <span class="n">basinhopping</span><span class="p">,</span> <span class="n">BHBounds</span><span class="p">,</span> <span class="n">BHRandStepBounded</span>
                <span class="n">bounds</span> <span class="o">=</span> <span class="n">BHBounds</span><span class="p">(</span><span class="n">xmin</span><span class="o">=</span><span class="n">ls_bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xmax</span><span class="o">=</span><span class="n">ls_bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">bounded_step</span> <span class="o">=</span> <span class="n">BHRandStepBounded</span><span class="p">(</span><span class="n">ls_bounds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ls_bounds</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">stepsize</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sol</span> <span class="o">=</span> <span class="n">basinhopping</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="p">(</span><span class="n">nlineq</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span>
                                        <span class="n">x0</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">),</span>
                                        <span class="n">T</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;T&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                        <span class="n">minimize_routine</span><span class="o">=</span><span class="n">basin_opt</span><span class="p">,</span>
                                        <span class="n">minimizer_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;saddle_point_equations&#39;</span><span class="p">:</span> <span class="n">nlineq</span> <span class="p">},</span>
                                        <span class="n">accept_test</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span>
                                        <span class="n">take_step</span><span class="o">=</span><span class="n">bounded_step</span><span class="p">,</span>
                                        <span class="n">niter</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;basin_hopping_niter&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                                        <span class="n">disp</span><span class="o">=</span><span class="nb">bool</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">)))</span>
        
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Only MLE and saddle_point methods are supported&#39;</span><span class="p">)</span>
        
        <span class="c1"># Compute the corrected Akaike information and Bayes information criteria</span>
        <span class="c1"># http://downloads.hindawi.com/journals/complexity/2019/5120581.pdf</span>
        <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">N</span><span class="o">*</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="c1"># n is the sample size</span>
        <span class="c1"># Both AIC and BIC are minimum for the best explanatory model</span>
        <span class="n">L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">loglikelihood</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="s1">&#39;AIC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">L</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">K</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">K</span><span class="o">*</span><span class="p">(</span><span class="n">K</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="n">K</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="s1">&#39;BIC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">L</span> <span class="o">+</span> <span class="n">K</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sol</span></div>


<div class="viewcode-block" id="ContinuousOptimizer"><a class="viewcode-back" href="../../../reference/generated/networkqit.algorithms.optimize.ContinuousOptimizer.html#networkqit.algorithms.optimize.ContinuousOptimizer">[docs]</a><span class="k">class</span> <span class="nc">ContinuousOptimizer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Continuos optimization method of spectral entropy in the continuous approximation S(rho, sigma(E[L])))</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ContinuousOptimizer.__init__"><a class="viewcode-back" href="../../../reference/generated/networkqit.algorithms.optimize.ContinuousOptimizer.html#networkqit.algorithms.optimize.ContinuousOptimizer.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">beta_range</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialization method, must provide the observed network in form of adjacency matrix,</span>
<span class="sd">        the initial optimization parameters and the range over which to span $\beta$.</span>

<span class="sd">        args:</span>
<span class="sd">            A (numpy.array): The observed adjacency matrix</span>
<span class="sd">            x0 (numpy.array): The initial value of the optimization parameters (also called θ_0)</span>
<span class="sd">            beta_range (numpy.array, list): The values for which to run optimization</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">A</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x0</span> <span class="o">=</span> <span class="n">x0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_range</span> <span class="o">=</span> <span class="n">beta_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">=</span> <span class="n">graph_laplacian</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_callback</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method computes the gradient as </span>
<span class="sd">        </span>
<span class="sd">        :math:`\\frac{s(\\boldsymbol \\rho \| \\boldsymbol \\sigma)}{\\partial \\boldsymbol \\theta_k} = \\beta \textrm{Tr}\left \lbrack \left( \rho - \sigma(\theta)\right) \frac{\mathbb{E}\mathbf{L}(\theta)}{\partial \theta_k} \right \rbrack`</span>
<span class="sd">        </span>
<span class="sd">        args:</span>
<span class="sd">            x (numpy.array): the current parameters</span>
<span class="sd">            rho (numpy.array): the observed density matrix</span>
<span class="sd">            beta (float): the beta, a positive real.</span>

<span class="sd">        Returns:</span>
<span class="sd">            the gradient as a three index numpy array. The last index is the one pertaining to the k-th component of x</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">compute_vonneuman_density</span><span class="p">(</span><span class="n">graph_laplacian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="n">beta</span><span class="p">)</span>
        <span class="c1"># Here instead of tracing over the matrix product, we just sum over the entrywise product of the two matrices</span>
        <span class="c1"># (rho-sigma) and the ∂E_θ[L]/.</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">rho</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_laplaplacian_grad</span><span class="p">(</span><span class="n">x</span><span class="p">)[:,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                         <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">))])</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Starts the optimization.</span>

<span class="sd">        Args:</span>
<span class="sd">            method (string): &#39;BFGS&#39;</span>
<span class="sd">            if the optimization problem is bounded, instead use one of the scipy constrained optimizer,</span>
<span class="sd">            which are &#39;L-BFGS-B&#39;, &#39;TNC&#39;, &#39;SLSQP&#39; or &#39;least_squares&#39;</span>
<span class="sd">        </span>
<span class="sd">        Kwargs:</span>

<span class="sd">            gtol: (float)</span>
<span class="sd">                gradient tolerance to be used in optimization (default 1E-12)</span>

<span class="sd">            maxfun (int):</span>
<span class="sd">                maximum number of function evaluations</span>

<span class="sd">            maxiter (int):</span>
<span class="sd">                maximum number of iterations of gradient descent</span>

<span class="sd">            xtol (float):</span>
<span class="sd">                tolerance in the solution change</span>

<span class="sd">        Output:</span>
<span class="sd">            sol: (scipy.optimize.OptimizeResult) parameters at optimal likelihood</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;method&#39;</span><span class="p">,</span> <span class="s1">&#39;BFGS&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bounds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">,</span> <span class="s1">&#39;TNC&#39;</span><span class="p">,</span> <span class="s1">&#39;SLSQP&#39;</span><span class="p">,</span> <span class="s1">&#39;least_squares&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">RuntimeWarning</span><span class="p">(</span><span class="s1">&#39;This model has bounds. BFGS cannot handle constraints nor bounds, switch to either &#39;</span>
                                 <span class="s1">&#39;L-BFGS-B, TNC or SLSQP&#39;</span><span class="p">)</span>

        <span class="c1"># Populate the solution list as function of beta</span>
        <span class="c1"># the list sol contains all optimization points</span>
        <span class="n">sol</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Iterate over all beta provided by the user</span>
        <span class="k">for</span> <span class="n">beta</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta_range</span><span class="p">:</span>
            <span class="c1"># define the relative entropy function, dependent on current beta</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">rel_entropy_fun</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">SpectralDivergence</span><span class="p">(</span><span class="n">Lobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">,</span>
                                                                <span class="n">Lmodel</span><span class="o">=</span><span class="n">graph_laplacian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span>
                                                                <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span><span class="o">.</span><span class="n">rel_entropy</span>

            <span class="c1"># Define initially fgrad as None, relying on numerical computation of it</span>
            <span class="n">fgrad</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># Define observed density rho as none initially, if necessary it is computed</span>
            <span class="n">rho</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># If user provides gradients of the model, use them, redefyining fgrad to pass to solver</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_laplaplacian_grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># compute rho for each beta, just once</span>
                <span class="n">rho</span> <span class="o">=</span> <span class="n">compute_vonneuman_density</span><span class="p">(</span><span class="n">L</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
                <span class="c1"># define the gradient of the Dkl, given rho at current beta</span>
                <span class="n">fgrad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

            <span class="c1"># Append the solutions</span>
            <span class="c1"># Here we adopt two different strategies, either minimizing the residual of gradients of Dkl</span>
            <span class="c1"># using least_squares or minimizing the Dkl itself.</span>
            <span class="c1"># The least_squares approach requires the gradients of the model, if they are not implemented </span>
            <span class="c1"># in the model, the algorithmic differentiation is used, which is slower</span>

            <span class="n">sol</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">minimize</span><span class="p">(</span><span class="n">fun</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rel_entropy_fun</span><span class="p">,</span>
                                <span class="n">x0</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">,</span>
                                <span class="n">jac</span><span class="o">=</span><span class="n">fgrad</span><span class="p">,</span>
                                <span class="n">method</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span>
                                <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;disp&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
                                         <span class="s1">&#39;gtol&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;gtol&#39;</span><span class="p">,</span> <span class="mf">1E-12</span><span class="p">),</span>
                                         <span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;maxiter&#39;</span><span class="p">,</span> <span class="mf">5E4</span><span class="p">),</span>
                                         <span class="s1">&#39;maxfun&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;maxfun&#39;</span><span class="p">,</span> <span class="mf">5E4</span><span class="p">)},</span>
                                <span class="n">bounds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bounds</span><span class="p">))</span>

            <span class="c1"># important to reinitialize from the last solution, solution is restarted at every step otherwise</span>
            <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;reinitialize&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">x0</span> <span class="o">=</span> <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">x</span>
            <span class="c1"># Call the step_callback function to print or display current solution</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_callback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">step_callback</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>

            <span class="c1"># Here creates the output data structure as a dictionary of the optimization parameters and variables</span>
            <span class="n">spect_div</span> <span class="o">=</span> <span class="n">SpectralDivergence</span><span class="p">(</span><span class="n">Lobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">,</span> <span class="n">Lmodel</span><span class="o">=</span><span class="n">graph_laplacian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="p">)),</span>
                                           <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
            <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;DeltaL&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">graph_laplacian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="p">))))</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;compute_sigma&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
                <span class="n">Lmodel</span> <span class="o">=</span> <span class="n">graph_laplacian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="p">))</span>
                <span class="n">rho</span> <span class="o">=</span> <span class="n">compute_vonneuman_density</span><span class="p">(</span><span class="n">L</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
                <span class="n">sigma</span> <span class="o">=</span> <span class="n">compute_vonneuman_density</span><span class="p">(</span><span class="n">L</span><span class="o">=</span><span class="n">Lmodel</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
                <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;&lt;DeltaL&gt;&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">rho</span><span class="o">-</span><span class="n">sigma</span><span class="p">)</span><span class="o">*</span><span class="n">Lmodel</span><span class="p">)</span>
            <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;T&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">beta</span>
            <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span>
            <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;loglike&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spect_div</span><span class="o">.</span><span class="n">loglike</span>
            <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;rel_entropy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spect_div</span><span class="o">.</span><span class="n">rel_entropy</span>
            <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;entropy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">spect_div</span><span class="o">.</span><span class="n">entropy</span>
            <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;AIC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;loglike&#39;</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)):</span>
                <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sol</span> <span class="o">=</span> <span class="n">sol</span>
        <span class="k">return</span> <span class="n">sol</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">to_dataframe</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A convenience function to summarize all the optimization process, with results of optimization.</span>

<span class="sd">        args:</span>
<span class="sd">            to_dataframe (bool): if True, returns a pandas dataframe, otherwise returns a list of dictionaries</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">to_dataframe</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
            <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">)</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;T&#39;</span><span class="p">)</span>  <span class="c1"># it&#39;s 1/beta</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{:&gt;20}</span><span class="s2"> &quot;</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Summary:&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model:</span><span class="se">\t</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">formula</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimization method: &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Variables bounds: &#39;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">):</span>
                <span class="n">left</span> <span class="o">=</span> <span class="s1">&#39;-∞&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">right</span> <span class="o">=</span> <span class="s1">&#39;+∞&#39;</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">bounds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{: &gt;1}</span><span class="s2"> </span><span class="si">{:}</span><span class="s2"> </span><span class="si">{: &gt;10}</span><span class="s2"> </span><span class="si">{:}</span><span class="s2"> </span><span class="si">{: &gt;1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">left</span><span class="p">,</span> <span class="s1">&#39;&lt;=&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s1">&#39;&lt;=&#39;</span><span class="p">,</span> <span class="n">right</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Results:&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Von Neumann Log Likelihood:</span><span class="se">\t</span><span class="s1">&#39;</span> <span class="o">+</span>
                  <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;loglike&#39;</span><span class="p">]))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Von Neumann Entropy:</span><span class="se">\t\t</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;entropy&#39;</span><span class="p">]))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Von Neumann Relative entropy:</span><span class="se">\t</span><span class="s1">&#39;</span> <span class="o">+</span>
                  <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;rel_entropy&#39;</span><span class="p">]))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;AIC:</span><span class="se">\t\t\t\t</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;AIC&#39;</span><span class="p">]))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Estimate:&#39;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;=&#39;</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">expected_adjacency</span><span class="o">.</span><span class="n">args_mapping</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">)):</span>
                <span class="n">row</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">x</span><span class="p">]</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;beta&#39;</span><span class="p">],</span> <span class="o">*</span><span class="n">row</span><span class="p">))</span></div>


<div class="viewcode-block" id="StochasticOptimizer"><a class="viewcode-back" href="../../../reference/generated/networkqit.algorithms.optimize.StochasticOptimizer.html#networkqit.algorithms.optimize.StochasticOptimizer">[docs]</a><span class="k">class</span> <span class="nc">StochasticOptimizer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class is at the base of implementation of methods based on stochastic gradient descent.</span>
<span class="sd">    The idea behind this class is to help the user in designing a nice stochastic gradient descent method,</span>
<span class="sd">    such as ADAM, AdaGrad or older methods, like the Munro-Robbins stochastic gradients optimizer.</span>
<span class="sd">    Working out the expression for the gradients of the relative entropy, one remains with the following:</span>

<span class="sd">    :math: `\nabla_{\theta}S(\rho \| \sigma) = \beta \textrm\biggl \lbrack \rho \nabla_{\theta}\mathbb{E}_{\theta}[L]} \biggr \rbrack`</span>
<span class="sd">        </span>
<span class="sd">    :math: `\frac{\partial S(\rho \| \sigma)}{\partial \theta_k} = \beta \textrm{Tr}\lbrack \rho \frac{\partial}{\partial \theta_k} \rbrack + \frac{\partial}{\partial \theta_k}\mathbb{E}_{\theta}\log \textrm{Tr} e^{-\beta L(\theta)}\lbrack \rbrack`</span>
<span class="sd">    </span>
<span class="sd">    This class requires either Tensorflow or Pytorch to support backpropagation in the eigenvalues routines.</span>
<span class="sd">    Alternatively you can use github.com/HIPS/autograd method for full CPU support.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="StochasticOptimizer.__init__"><a class="viewcode-back" href="../../../reference/generated/networkqit.algorithms.optimize.StochasticOptimizer.html#networkqit.algorithms.optimize.StochasticOptimizer.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">G</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">x0</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">model</span> <span class="p">:</span> <span class="n">GraphModel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the stochastic optimizer with the observed graph, an initial guess and the </span>
<span class="sd">        model to optimize.</span>

<span class="sd">        Args:</span>
<span class="sd">            G (numpy.array) :is the empirical network to study. A N x N adjacency matrix as numpy.array.</span>
<span class="sd">            x0 (numpy.array): is the k-element array of initial parameters estimates. Typically set as random.</span>
<span class="sd">            model (nq.GraphModel): the graph model to optimize the likelihood of.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">G</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">=</span> <span class="n">graph_laplacian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">)</span> <span class="c1"># compute graph laplacian</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x0</span> <span class="o">=</span> <span class="n">x0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sol</span> <span class="o">=</span> <span class="n">OptimizeResult</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">,</span>
                                  <span class="n">success</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                  <span class="n">status</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">message</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span>
                                  <span class="n">fun</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">jac</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">hess</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">hess_inv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                  <span class="n">nfev</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># number of function evaluations</span>
                                  <span class="n">njev</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># number of jacobian evaluations</span>
                                  <span class="n">nhev</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="c1"># number of hessian evaluations</span>
                                  <span class="n">nit</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># number of iterations</span>
                                  <span class="n">maxcv</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># maximum constraint violation</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ls_bounds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">bounds</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ls_bounds</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ls_bounds</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">min_bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ls_bounds</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">),</span><span class="mi">2</span><span class="p">])[:,</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">max_bounds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">ls_bounds</span><span class="o">.</span><span class="n">reshape</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x0</span><span class="p">),</span><span class="mi">2</span><span class="p">])[:,</span><span class="mi">1</span><span class="p">]</span></div>


    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Compute the relative entropy between rho and sigma, using tensors with autograd support</span>
        <span class="c1"># Entropy of rho is constant over the iterations</span>
        <span class="c1"># if beta is kept constant, otherwise a specific rho can be passed</span>
        <span class="c1">#lambd_rho = eigh(rho)[0]</span>
        <span class="c1">#entropy = -np.sum(lambd_rho * np.log(lambd_rho))</span>

        <span class="k">def</span> <span class="nf">log_likelihood</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
            <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="p">)</span>
            <span class="c1"># advanced broadcasting here!</span>
            <span class="c1"># Sample &#39;batch_size&#39; adjacency matrices shape=[batch_size,N,N]</span>
            <span class="n">Amodel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">sample_adjacency</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">with_grads</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1">#print(&#39;Amodel nan?:&#39;, np.any(np.isnan(Amodel.ravel())))</span>
            <span class="c1"># Here exploit broadcasting to create batch_size diagonal matrices with the degrees</span>
            <span class="n">Dmodel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">])</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ijk-&gt;ik&#39;</span><span class="p">,</span> <span class="n">Amodel</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
            <span class="n">Lmodel</span> <span class="o">=</span> <span class="n">Dmodel</span> <span class="o">-</span> <span class="n">Amodel</span>  <span class="c1"># returns a batch_size x N x N tensor</span>
            <span class="c1"># do average over batches of the sum of product of matrix elements (done with * operator)</span>
            <span class="n">Emodel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Lmodel</span> <span class="o">*</span> <span class="n">rho</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">lambd_model</span> <span class="o">=</span> <span class="n">eigh</span><span class="p">(</span><span class="n">Lmodel</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># eigh is a batched operation, take the eigenvalues only</span>
            <span class="n">Fmodel</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">logsumexp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span> <span class="o">*</span> <span class="n">lambd_model</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">beta</span><span class="p">)</span>
            <span class="n">loglike</span> <span class="o">=</span> <span class="o">-</span><span class="n">beta</span> <span class="o">*</span> <span class="n">Emodel</span> <span class="o">+</span> <span class="n">Fmodel</span> <span class="c1"># - Tr[rho log(sigma)]</span>
            <span class="c1">#loglike  #- entropy # Tr[rho log(rho)] - Tr[rho log(sigma)]</span>
            
            <span class="c1"># Add a penalty term accouting for boundaries violation</span>
            <span class="c1"># Follows these ideas https://www.cs.jhu.edu/~ijwang/pub/01271742.pdf</span>
            <span class="k">def</span> <span class="nf">quadratic_penalty</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
                <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="c1"># compute the penalty with respect to low bounds violation </span>
                <span class="n">penalty_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">z</span><span class="p">,</span> <span class="o">-</span><span class="n">theta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">min_bounds</span><span class="p">]))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                <span class="c1"># compute the penalty with respect to high bounds violation </span>
                <span class="n">penalty_high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">z</span><span class="p">,</span> <span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">max_bounds</span><span class="p">]))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
                <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">penalty_low</span> <span class="o">+</span> <span class="n">penalty_high</span><span class="p">)</span>
            <span class="k">def</span> <span class="nf">absolute_value_penalty</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
                <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                <span class="c1"># compute the penalty with respect to low bounds violation </span>
                <span class="n">penalty_low</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">z</span><span class="p">,</span> <span class="o">-</span><span class="n">theta</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">min_bounds</span><span class="p">]))</span>
                <span class="c1"># compute the penalty with respect to high bounds violation </span>
                <span class="n">penalty_high</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">z</span><span class="p">,</span> <span class="n">theta</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">max_bounds</span><span class="p">]))</span>
                <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(</span><span class="n">penalty_low</span><span class="p">,</span><span class="n">penalty_high</span><span class="p">))</span>
            <span class="n">penalty</span> <span class="o">=</span> <span class="n">quadratic_penalty</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">loglike</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">penalty</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">cost</span>

        <span class="c1"># value and gradient of relative entropy as a function</span>
        <span class="n">dkl_and_dkldx</span> <span class="o">=</span> <span class="n">autograd</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">log_likelihood</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dkl_and_dkldx</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>


<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../reference/generated/networkqit.algorithms.optimize.Adam.html#networkqit.algorithms.optimize.Adam">[docs]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">StochasticOptimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the ADAM stochastic gradient descent.</span>
<span class="sd">    Adam: A Method for Stochastic Optimization</span>
<span class="sd">    Diederik P. Kingma, Jimmy Ba</span>

<span class="sd">    https://arxiv.org/abs/1412.6980</span>
<span class="sd">    However here we use quasi-hyperbolic adam by default, with parameters nu1,nu2</span>
<span class="sd">    https://arxiv.org/pdf/1810.06801.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="Adam.__init__"><a class="viewcode-back" href="../../../reference/generated/networkqit.algorithms.optimize.Adam.html#networkqit.algorithms.optimize.Adam.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">G</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">x0</span> <span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">model</span> <span class="p">:</span> <span class="n">GraphModel</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logfile</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;adam.log&#39;</span><span class="p">,</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="o">.</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">beta</span><span class="p">,</span>
            <span class="n">rho</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">maxiter</span><span class="o">=</span><span class="mf">1E4</span><span class="p">,</span>
            <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1E-3</span><span class="p">,</span>
            <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
            <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">,</span>
            <span class="n">nu1</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
            <span class="n">nu2</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">epsilon</span><span class="o">=</span><span class="mf">1E-8</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">ftol</span><span class="o">=</span><span class="mf">1E-10</span><span class="p">,</span>
            <span class="n">gtol</span><span class="o">=</span><span class="mf">1E-5</span><span class="p">,</span>
            <span class="n">xtol</span><span class="o">=</span><span class="mf">1E-8</span><span class="p">,</span>
            <span class="n">last_iters</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
            <span class="n">quasi_hyperbolic</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
            <span class="n">callback</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        
        <span class="n">opts</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;ftol&#39;</span><span class="p">:</span> <span class="n">ftol</span><span class="p">,</span>
                <span class="s1">&#39;gtol&#39;</span><span class="p">:</span> <span class="n">gtol</span><span class="p">,</span> <span class="c1"># as default of LBFGSB</span>
                <span class="s1">&#39;xtol&#39;</span><span class="p">:</span> <span class="n">xtol</span><span class="p">,</span>
                <span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="n">maxiter</span><span class="p">,</span>
                <span class="s1">&#39;eps&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="mf">1E-8</span><span class="p">),</span> <span class="c1"># as default of LBFGSB</span>
                <span class="s1">&#39;maxfun&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;maxfun&#39;</span><span class="p">,</span> <span class="mf">1E10</span><span class="p">),</span>
                <span class="s1">&#39;verbose&#39;</span> <span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
                <span class="s1">&#39;disp&#39;</span><span class="p">:</span>  <span class="nb">bool</span><span class="p">(</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
                <span class="s1">&#39;iprint&#39;</span><span class="p">:</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;verbose&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                <span class="p">}</span>

        <span class="c1"># if rho is provided, user rho is used, otherwise is computed at every beta</span>
        <span class="k">if</span> <span class="n">rho</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">rho</span> <span class="o">=</span> <span class="n">compute_vonneuman_density</span><span class="p">(</span><span class="n">L</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
        
        <span class="c1">#logging.basicConfig(stream=sys.stderr, level=logging.INFO)</span>
        <span class="c1">#import logging</span>
        <span class="c1">#adam_logger = logging.getLogger(&#39;ADAM&#39;)</span>
        <span class="c1">#adam_logger.setLevel(logging.INFO)</span>
        <span class="c1">#all_dkl = []</span>
        <span class="c1">#from scipy.stats import linregress</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">maxiter</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="o">.</span><span class="n">nit</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># get the relative entropy value and its gradient w.r.t. variables</span>
            <span class="n">dkl</span><span class="p">,</span> <span class="n">grad_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">rho</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\r</span><span class="s1">Iteration </span><span class="si">%d</span><span class="s1"> beta=</span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">beta</span><span class="p">),</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
            <span class="c1"># Convergence status</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad_t</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">gtol</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="o">.</span><span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="o">.</span><span class="n">message</span> <span class="o">=</span> <span class="s1">&#39;Exceeded minimum gradient |grad| &lt; </span><span class="si">%g</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">gtol</span>
                <span class="k">break</span>
            <span class="c1">#all_dkl.append(dkl)</span>
            <span class="c1"># check the convergence based on the slope of dkl of the last 10 iterations</span>
            <span class="c1">#if t &gt; last_iters:</span>
            <span class="c1">#    dkl_iters = np.arange(last_iters)</span>
            <span class="c1">#    slope, intercept, r_value, p_value, std_err = linregress(x=dkl_iters, y=all_dkl[-last_iters:])</span>
            <span class="c1">#    if np.abs(slope - 1E-3) &lt; 0:</span>
            <span class="c1">#        converged = True</span>
            <span class="c1">#        print(&#39;Optimization terminated for flatness&#39;)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">mt</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mt</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad_t</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vt</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vt</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_t</span> <span class="o">*</span> <span class="n">grad_t</span><span class="p">)</span>
            <span class="n">mttilde</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mt</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="n">beta1</span> <span class="o">**</span> <span class="n">t</span><span class="p">))</span>  <span class="c1"># compute bias corrected first moment estimate</span>
            <span class="n">vttilde</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vt</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="n">beta2</span> <span class="o">**</span> <span class="n">t</span><span class="p">))</span>  <span class="c1"># compute bias-corrected second raw moment estimate</span>
            <span class="k">if</span> <span class="n">quasi_hyperbolic</span><span class="p">:</span> <span class="c1"># quasi hyperbolic adam</span>
                <span class="n">deltax</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">nu1</span><span class="p">)</span> <span class="o">*</span> <span class="n">grad_t</span> <span class="o">+</span> <span class="n">nu1</span> <span class="o">*</span> <span class="n">mttilde</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">nu2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">grad_t</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">nu2</span> <span class="o">*</span> <span class="n">vttilde</span> <span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span> <span class="c1"># vanilla Adam</span>
                <span class="n">deltax</span> <span class="o">=</span> <span class="n">mttilde</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vttilde</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="o">.</span><span class="n">x</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">deltax</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logfile</span><span class="o">.</span><span class="n">write</span><span class="p">(</span> <span class="sa">u</span><span class="s2">&quot;</span><span class="si">%g</span><span class="se">\t</span><span class="si">%g</span><span class="se">\t</span><span class="si">%g</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sol</span><span class="o">.</span><span class="n">x</span><span class="p">,</span><span class="n">beta</span><span class="p">,</span><span class="n">dkl</span><span class="p">)</span> <span class="p">)</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">50</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logfile</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sol</span></div>

    <span class="c1"># def run(self, **kwargs):</span>
    <span class="c1">#     x = self.x0</span>
    <span class="c1">#     batch_size = kwargs.get(&#39;batch_size&#39;, 1)</span>
    <span class="c1">#     max_iters = kwargs.get(&#39;max_iters&#39;, 1000)</span>
    <span class="c1">#     # ADAM parameters</span>
    <span class="c1">#     eta = kwargs.get(&#39;eta&#39;, 1E-3)</span>
    <span class="c1">#     gtol = kwargs.get(&#39;gtol&#39;, 1E-4)</span>
    <span class="c1">#     xtol = kwargs.get(&#39;xtol&#39;, 1E-3)</span>
    <span class="c1">#     beta1 = 0.9</span>
    <span class="c1">#     beta2 = 0.999</span>
    <span class="c1">#     epsilon = 1E-8 # avoid nan with large learning rates</span>
    <span class="c1">#     # Use quasi-hyperbolic adam by default</span>
    <span class="c1">#     # https://arxiv.org/pdf/1810.06801.pdf</span>
    <span class="c1">#     quasi_hyperbolic = kwargs.get(&#39;quasi_hyperbolic&#39;, True)</span>
    <span class="c1">#     nu1 = 0.7 # for quasi-hyperbolic adam</span>
    <span class="c1">#     nu2 = 1.0 # for quasi-hyperbolic adam</span>
    <span class="c1">#     # visualization options</span>
    <span class="c1">#     refresh_frames = kwargs.get(&#39;refresh_frames&#39;, 100)</span>
    <span class="c1">#     from drawnow import drawnow, figure</span>
    <span class="c1">#     figure(figsize=(8,8))</span>
    <span class="c1">#     import matplotlib.pyplot as plt</span>
    <span class="c1">#     #plt.figure(figsize=(8, 8))</span>

    <span class="c1">#     # Populate the solution list as function of beta</span>
    <span class="c1">#     # the list sol contains all optimization points</span>
    <span class="c1">#     sol = []</span>
    <span class="c1">#     # Iterate over all beta provided by the user</span>
    <span class="c1">#     mt, vt = np.zeros(self.x0.shape), np.zeros(self.x0.shape)</span>
    <span class="c1">#     all_dkl = []</span>
        
    <span class="c1">#     # TODO implement model boundaries in Adam</span>
    <span class="c1">#     frames = 0</span>
    <span class="c1">#     filename = &#39;adam.log&#39;</span>
    <span class="c1">#     logfile = open(filename,&#39;w&#39;)</span>
    <span class="c1">#     from scipy.stats import linregress</span>
    <span class="c1">#     import logging</span>
    <span class="c1">#     import sys</span>
    <span class="c1">#     logging.basicConfig(stream=sys.stderr, level=logging.INFO)</span>
    <span class="c1">#     adam_logger = logging.getLogger(&#39;ADAM&#39;)</span>
    <span class="c1">#     adam_logger.setLevel(logging.INFO)</span>

    <span class="c1">#     for beta in self.beta_range:</span>
    <span class="c1">#         adam_logger.info(&#39;Changed beta to %g&#39; % beta)</span>
    <span class="c1">#         # if rho is provided, user rho is used, otherwise is computed at every beta</span>
    <span class="c1">#         rho = kwargs.get(&#39;rho&#39;, compute_vonneuman_density(L=self.L, beta=beta))</span>
    <span class="c1">#         # initialize at x0</span>
    <span class="c1">#         x = self.x0</span>
    <span class="c1">#         converged = False</span>
    <span class="c1">#         t = 0  # t is the iteration number</span>
    <span class="c1">#         while not converged:</span>
    <span class="c1">#             t += 1</span>
    <span class="c1">#             # get the relative entropy value and its gradient w.r.t. variables</span>
    <span class="c1">#             dkl, grad_t = self.gradient(x, rho, beta, batch_size=batch_size)</span>
    <span class="c1">#             # Convergence status</span>
    <span class="c1">#             if np.linalg.norm(grad_t) &lt; gtol:</span>
    <span class="c1">#                 converged = True</span>
    <span class="c1">#                 adam_logger.info(&#39;Exceeded minimum gradient |grad|&lt;%g&#39; % gtol)</span>
                
    <span class="c1">#             if t &gt; max_iters:</span>
    <span class="c1">#                 adam_logger.info(&#39;Exceeded maximum iterations&#39;)</span>
    <span class="c1">#                 converged = True</span>

    <span class="c1">#             # check the convergence based on the slope of dkl of the last 10 iterations</span>
    <span class="c1">#             last_iters = 100</span>
    <span class="c1">#             if len(all_dkl) &gt; last_iters:</span>
    <span class="c1">#                 dkl_iters = np.arange(last_iters)</span>
    <span class="c1">#                 slope, intercept, r_value, p_value, std_err = linregress(x=dkl_iters, y=all_dkl[-last_iters:])</span>
    <span class="c1">#                 if np.abs(slope - 1E-3) &lt; 0:</span>
    <span class="c1">#                         converged = True</span>
    <span class="c1">#                         adam_logger.info(&#39;Optimization terminated for flatness&#39;)</span>

    <span class="c1">#             # TODO implement check boundaries in Adam</span>
    <span class="c1">#             # if np.any(np.ravel(self.model.bounds)):</span>
    <span class="c1">#             #    raise RuntimeError(&#39;variable bounds exceeded&#39;)</span>
    <span class="c1">#             mt = beta1 * mt + (1.0 - beta1) * grad_t</span>
    <span class="c1">#             vt = beta2 * vt + (1.0 - beta2) * (grad_t * grad_t)</span>
    <span class="c1">#             mttilde = mt / (1.0 - (beta1 ** t))  # compute bias corrected first moment estimate</span>
    <span class="c1">#             vttilde = vt / (1.0 - (beta2 ** t))  # compute bias-corrected second raw moment estimate</span>
    <span class="c1">#             if quasi_hyperbolic: # quasi hyperbolic adam</span>
    <span class="c1">#                 deltax = ((1-nu1) * grad_t + nu1 * mttilde)/(np.sqrt((1-nu2) * (grad_t**2) + nu2 * vttilde ) + epsilon)</span>
    <span class="c1">#             else: # vanilla Adam</span>
    <span class="c1">#                 deltax = mttilde / np.sqrt(vttilde + epsilon)</span>
    <span class="c1">#             x -= eta * deltax</span>
    <span class="c1">#             #all_dkl.append(dkl)</span>
    <span class="c1">#             #print(np.hstack([t, x,beta,dkl]))</span>
    <span class="c1">#             #logfile.write( u&quot;%g\t%g\t%g\n&quot; % (x,beta,dkl) )</span>
    <span class="c1">#             # if t % refresh_frames == 0:</span>
    <span class="c1">#             #     frames += 1</span>
    <span class="c1">#             #     def draw_fig():</span>
    <span class="c1">#             #         plot_beta_range = np.logspace(-3,3,100)</span>
    <span class="c1">#             #         sol.append({&#39;x&#39;: x.copy()})</span>
    <span class="c1">#             #         #plt.figure(figsize=(8, 8))</span>
    <span class="c1">#             #         A0 = np.mean(self.model.sample_adjacency(theta=x, batch_size=batch_size), axis=0)</span>
    <span class="c1">#             #         plt.subplot(2, 2, 1)</span>
    <span class="c1">#             #         im = plt.imshow(self.A)</span>
    <span class="c1">#             #         plt.colorbar(im)</span>
    <span class="c1">#             #         plt.title(&#39;Data&#39;)</span>
    <span class="c1">#             #         plt.subplot(2, 2, 2)</span>
    <span class="c1">#             #         im = plt.imshow(A0)</span>
    <span class="c1">#             #         plt.colorbar(im)</span>
    <span class="c1">#             #         plt.title(&#39;&lt;Model&gt;&#39;)</span>
    <span class="c1">#             #         plt.subplot(2, 2, 3)</span>
    <span class="c1">#             #         plt.plot(all_dkl)</span>
    <span class="c1">#             #         plt.xlabel(&#39;iteration&#39;)</span>
    <span class="c1">#             #         plt.ylabel(&#39;$S(\\boldsymbol \\rho,\\boldsymbol \\sigma)$&#39;)</span>
    <span class="c1">#             #         plt.subplot(2, 2, 4)</span>
    <span class="c1">#             #         plt.semilogx(plot_beta_range, batch_compute_vonneumann_entropy(self.L, plot_beta_range), &#39;.-&#39;, label=&#39;data&#39;)</span>
    <span class="c1">#             #         plt.semilogx(plot_beta_range, batch_compute_vonneumann_entropy(graph_laplacian(A0), plot_beta_range), &#39;.-&#39;, label=&#39;model&#39;)</span>
    <span class="c1">#             #         plt.plot(beta, batch_compute_vonneumann_entropy(graph_laplacian(A0), [beta]), &#39;ko&#39;, label=&#39;model&#39;)</span>
    <span class="c1">#             #         plt.xlabel(&#39;$\\beta$&#39;)</span>
    <span class="c1">#             #         plt.ylabel(&#39;$S$&#39;)</span>
    <span class="c1">#             #         plt.title(&#39;Entropy&#39;)</span>
    <span class="c1">#             #         plt.legend(loc=&#39;best&#39;)</span>
    <span class="c1">#             #         plt.suptitle(&#39;$\\beta=$&#39; + &#39;{0:0&gt;3}&#39;.format(beta))</span>
    <span class="c1">#             #         #plt.tight_layout()</span>
    <span class="c1">#             #     drawnow(draw_fig)</span>
    <span class="c1">#     self.sol = sol</span>
    <span class="c1">#     adam_logger.info(&#39;Optimization finished\n&#39; + str(self.sol))</span>
    <span class="c1">#     return sol</span>

</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2019, networkqit Developers
      <span class="lastupdated">
        Last updated on Apr 16, 2019.
      </span>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>